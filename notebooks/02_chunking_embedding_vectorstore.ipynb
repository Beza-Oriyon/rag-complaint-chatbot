{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dddee635",
   "metadata": {},
   "source": [
    "Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3db67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a9e8a",
   "metadata": {},
   "source": [
    "Load filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4481beb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target per category: {'Credit Cards': 4836, 'Savings Accounts': 3960, 'Money Transfers': 2520, 'Personal Loans': 684}\n",
      "Starting chunked sampling...\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     27\u001b[39m collected = {cat: \u001b[32m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cat \u001b[38;5;129;01min\u001b[39;00m target_per_category}\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting chunked sampling...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcomplaint_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_category\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcleaned_narrative\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_per_category\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollected\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_per_category\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bezaw\\OneDrive\\Desktop\\10Acadamy-KAIM\\RAG-Compliant-Chatbot\\rag-complaint-chatbot\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1843\u001b[39m, in \u001b[36mTextFileReader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1841\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DataFrame:\n\u001b[32m   1842\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1843\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1844\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   1845\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bezaw\\OneDrive\\Desktop\\10Acadamy-KAIM\\RAG-Compliant-Chatbot\\rag-complaint-chatbot\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1985\u001b[39m, in \u001b[36mTextFileReader.get_chunk\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m   1983\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m   1984\u001b[39m     size = \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m.nrows - \u001b[38;5;28mself\u001b[39m._currow)\n\u001b[32m-> \u001b[39m\u001b[32m1985\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bezaw\\OneDrive\\Desktop\\10Acadamy-KAIM\\RAG-Compliant-Chatbot\\rag-complaint-chatbot\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bezaw\\OneDrive\\Desktop\\10Acadamy-KAIM\\RAG-Compliant-Chatbot\\rag-complaint-chatbot\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:850\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "csv_path = 'data/processed/filtered_complaints.csv'\n",
    "sample_size = 12000  # Target ~12K\n",
    "\n",
    "# We'll collect complaint_id and cleaned_narrative per category\n",
    "samples = defaultdict(list)\n",
    "\n",
    "# Desired proportions based on your known distribution (~40% CC, 33% SA, 21% MT, 6% PL)\n",
    "proportions = {\n",
    "    'Credit Cards': 0.403,   # 189334 / 469922 ≈ 0.403\n",
    "    'Savings Accounts': 0.330,\n",
    "    'Money Transfers': 0.210,\n",
    "    'Personal Loans': 0.057\n",
    "}\n",
    "\n",
    "target_per_category = {cat: int(sample_size * prop) for cat, prop in proportions.items()}\n",
    "print(\"Target per category:\", target_per_category)\n",
    "\n",
    "chunksize = 50_000  # Smaller chunks to stay safe\n",
    "collected = {cat: 0 for cat in target_per_category}\n",
    "\n",
    "print(\"Starting chunked sampling...\")\n",
    "\n",
    "for chunk in pd.read_csv(csv_path, usecols=['complaint_id', 'product_category', 'cleaned_narrative'], chunksize=chunksize):\n",
    "    for cat in target_per_category:\n",
    "        if collected[cat] >= target_per_category[cat]:\n",
    "            continue  # Already enough for this category\n",
    "        \n",
    "        cat_mask = chunk['product_category'] == cat\n",
    "        cat_chunk = chunk[cat_mask]\n",
    "        \n",
    "        needed = target_per_category[cat] - collected[cat]\n",
    "        if len(cat_chunk) > needed:\n",
    "            cat_chunk = cat_chunk.sample(n=needed, random_state=42)\n",
    "        \n",
    "        # Store as dicts to save memory\n",
    "        for _, row in cat_chunk.iterrows():\n",
    "            samples[cat].append({\n",
    "                'complaint_id': row['complaint_id'],\n",
    "                'product_category': cat,\n",
    "                'cleaned_narrative': row['cleaned_narrative']\n",
    "            })\n",
    "        \n",
    "        collected[cat] += len(cat_chunk)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Collected: {dict(collected)}\")\n",
    "\n",
    "# Combine all samples\n",
    "all_samples = []\n",
    "for cat_list in samples.values():\n",
    "    all_samples.extend(cat_list)\n",
    "\n",
    "# Shuffle final sample\n",
    "random.shuffle(all_samples)\n",
    "\n",
    "df_sample = pd.DataFrame(all_samples)\n",
    "\n",
    "print(f\"\\nFinal sample size: {len(df_sample)}\")\n",
    "print(\"Distribution:\")\n",
    "print(df_sample['product_category'].value_counts())\n",
    "\n",
    "# Save small sample for Task 2\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "df_sample.to_csv('data/processed/sample_12k_complaints.csv', index=False)\n",
    "print(\"\\nSaved small sample to data/processed/sample_12k_complaints.csv\")\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e666aa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sample: 12000 complaints\n",
      "product_category\n",
      "Credit Cards        4836\n",
      "Savings Accounts    3960\n",
      "Money Transfers     2520\n",
      "Personal Loans       684\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample narrative example:\n",
      "it was an application for the xx/xx/ credit card i never finished to changed my mind, i never signed anything for a credit pull, the app started with some undisclosed pre - something i never seen any written, and i definitely did not physically sign for anything! i tried very hard to make a good credit score and i was doing real good and now, they jacked it all up. , in, i believe it was on the , elan financial, xx/xx/. thank you!...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaint_id</th>\n",
       "      <th>product_category</th>\n",
       "      <th>cleaned_narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10055116</td>\n",
       "      <td>Credit Cards</td>\n",
       "      <td>it was an application for the xx/xx/ credit ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11657736</td>\n",
       "      <td>Credit Cards</td>\n",
       "      <td>i need your help to review accounts from compa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11771646</td>\n",
       "      <td>Savings Accounts</td>\n",
       "      <td>i am writing to file a complaint against navy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11221539</td>\n",
       "      <td>Credit Cards</td>\n",
       "      <td>the company must ensure a transparent and effi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11940104</td>\n",
       "      <td>Credit Cards</td>\n",
       "      <td>on or around xx/xx/year&gt;, i called the chase ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   complaint_id  product_category  \\\n",
       "0      10055116      Credit Cards   \n",
       "1      11657736      Credit Cards   \n",
       "2      11771646  Savings Accounts   \n",
       "3      11221539      Credit Cards   \n",
       "4      11940104      Credit Cards   \n",
       "\n",
       "                                   cleaned_narrative  \n",
       "0  it was an application for the xx/xx/ credit ca...  \n",
       "1  i need your help to review accounts from compa...  \n",
       "2  i am writing to file a complaint against navy ...  \n",
       "3  the company must ensure a transparent and effi...  \n",
       "4  on or around xx/xx/year>, i called the chase ,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_sample = pd.read_csv('data/processed/sample_12k_complaints.csv')\n",
    "\n",
    "print(f\"Loaded sample: {len(df_sample)} complaints\")\n",
    "print(df_sample['product_category'].value_counts())\n",
    "print(\"\\nSample narrative example:\")\n",
    "print(df_sample['cleaned_narrative'].iloc[0][:500] + \"...\")\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8994fc0",
   "metadata": {},
   "source": [
    "Text Chunking with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c267c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example narrative split into 1 chunks\n",
      "\n",
      "Chunk 1 (434 chars):\n",
      "it was an application for the xx/xx/ credit card i never finished to changed my mind, i never signed anything for a credit pull, the app started with some undisclosed pre - something i never seen any written, and i definitely did not physically sign for anything! i tried very hard to make a good cre...\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# As per pre-built vector store specs\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,  # character-based\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Test on one narrative\n",
    "example_text = df_sample['cleaned_narrative'].iloc[0]\n",
    "chunks = text_splitter.split_text(example_text)\n",
    "\n",
    "print(f\"Example narrative split into {len(chunks)} chunks\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(chunk[:300] + \"...\" if len(chunk) > 300 else chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9cffb9",
   "metadata": {},
   "source": [
    "Create Chunks with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b836e26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking complaints: 100%|██████████| 12000/12000 [00:21<00:00, 568.48it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 32060 chunks from 12000 complaints\n",
      "Average chunks per complaint: 2.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "chunks_with_metadata = []\n",
    "\n",
    "for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Chunking complaints\"):\n",
    "    narrative = row['cleaned_narrative']\n",
    "    complaint_id = str(row['complaint_id'])\n",
    "    product_category = row['product_category']\n",
    "    \n",
    "    # Split text\n",
    "    text_chunks = text_splitter.split_text(narrative)\n",
    "    \n",
    "    # Add metadata to each chunk\n",
    "    for chunk_idx, chunk_text in enumerate(text_chunks):\n",
    "        chunks_with_metadata.append({\n",
    "            'text': chunk_text,\n",
    "            'complaint_id': complaint_id,\n",
    "            'product_category': product_category,\n",
    "            'chunk_index': chunk_idx,\n",
    "            'total_chunks': len(text_chunks)\n",
    "        })\n",
    "\n",
    "print(f\"\\nCreated {len(chunks_with_metadata)} chunks from {len(df_sample)} complaints\")\n",
    "print(f\"Average chunks per complaint: {len(chunks_with_metadata)/len(df_sample):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf367e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks saved to data/processed/chunks_with_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save chunks to file\n",
    "with open('data/processed/chunks_with_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(chunks_with_metadata, f)\n",
    "\n",
    "print(\"Chunks saved to data/processed/chunks_with_metadata.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9979fc",
   "metadata": {},
   "source": [
    "Generate Embeddings and Build ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf66786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded chunks_with_metadata from pickle file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezaw\\OneDrive\\Desktop\\10Acadamy-KAIM\\RAG-Compliant-Chatbot\\rag-complaint-chatbot\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection has 8000 vectors before adding\n",
      "Starting batch insertion...\n",
      "Adding batch 1: chunks 0 to 199\n",
      "Adding batch 2: chunks 200 to 399\n",
      "Adding batch 3: chunks 400 to 599\n",
      "Adding batch 4: chunks 600 to 799\n",
      "Adding batch 5: chunks 800 to 999\n",
      "Adding batch 6: chunks 1000 to 1199\n",
      "Adding batch 7: chunks 1200 to 1399\n",
      "Adding batch 8: chunks 1400 to 1599\n",
      "Adding batch 9: chunks 1600 to 1799\n",
      "Adding batch 10: chunks 1800 to 1999\n",
      "Adding batch 11: chunks 2000 to 2199\n",
      "Adding batch 12: chunks 2200 to 2399\n",
      "Adding batch 13: chunks 2400 to 2599\n",
      "Adding batch 14: chunks 2600 to 2799\n",
      "Adding batch 15: chunks 2800 to 2999\n",
      "Adding batch 16: chunks 3000 to 3199\n",
      "Adding batch 17: chunks 3200 to 3399\n",
      "Adding batch 18: chunks 3400 to 3599\n",
      "Adding batch 19: chunks 3600 to 3799\n",
      "Adding batch 20: chunks 3800 to 3999\n",
      "Adding batch 21: chunks 4000 to 4199\n",
      "Adding batch 22: chunks 4200 to 4399\n",
      "Adding batch 23: chunks 4400 to 4599\n",
      "Adding batch 24: chunks 4600 to 4799\n",
      "Adding batch 25: chunks 4800 to 4999\n",
      "Adding batch 26: chunks 5000 to 5199\n",
      "Adding batch 27: chunks 5200 to 5399\n",
      "Adding batch 28: chunks 5400 to 5599\n",
      "Adding batch 29: chunks 5600 to 5799\n",
      "Adding batch 30: chunks 5800 to 5999\n",
      "Adding batch 31: chunks 6000 to 6199\n",
      "Adding batch 32: chunks 6200 to 6399\n",
      "Adding batch 33: chunks 6400 to 6599\n",
      "Adding batch 34: chunks 6600 to 6799\n",
      "Adding batch 35: chunks 6800 to 6999\n",
      "Adding batch 36: chunks 7000 to 7199\n",
      "Adding batch 37: chunks 7200 to 7399\n",
      "Adding batch 38: chunks 7400 to 7599\n",
      "Adding batch 39: chunks 7600 to 7799\n",
      "Adding batch 40: chunks 7800 to 7999\n",
      "Adding batch 41: chunks 8000 to 8199\n",
      "Adding batch 42: chunks 8200 to 8399\n",
      "Adding batch 43: chunks 8400 to 8599\n",
      "Adding batch 44: chunks 8600 to 8799\n",
      "Adding batch 45: chunks 8800 to 8999\n",
      "Adding batch 46: chunks 9000 to 9199\n",
      "Adding batch 47: chunks 9200 to 9399\n",
      "Adding batch 48: chunks 9400 to 9599\n",
      "Adding batch 49: chunks 9600 to 9799\n",
      "Adding batch 50: chunks 9800 to 9999\n",
      "Adding batch 51: chunks 10000 to 10199\n",
      "Adding batch 52: chunks 10200 to 10399\n",
      "Adding batch 53: chunks 10400 to 10599\n",
      "Adding batch 54: chunks 10600 to 10799\n",
      "Adding batch 55: chunks 10800 to 10999\n",
      "Adding batch 56: chunks 11000 to 11199\n",
      "Adding batch 57: chunks 11200 to 11399\n",
      "Adding batch 58: chunks 11400 to 11599\n",
      "Adding batch 59: chunks 11600 to 11799\n",
      "Adding batch 60: chunks 11800 to 11999\n",
      "Adding batch 61: chunks 12000 to 12199\n",
      "Adding batch 62: chunks 12200 to 12399\n",
      "Adding batch 63: chunks 12400 to 12599\n",
      "Adding batch 64: chunks 12600 to 12799\n",
      "Adding batch 65: chunks 12800 to 12999\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Error in compaction: Failed to apply logs to the metadata segment",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m end = \u001b[38;5;28mmin\u001b[39m(start + batch_size, \u001b[38;5;28mlen\u001b[39m(ids))\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAdding batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart//batch_size\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: chunks \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend-\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mcollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m total_added += (end - start)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Force garbage collection\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bezaw\\OneDrive\\Desktop\\10Acadamy-KAIM\\RAG-Compliant-Chatbot\\rag-complaint-chatbot\\venv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:97\u001b[39m, in \u001b[36mCollection.add\u001b[39m\u001b[34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m    ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     85\u001b[39m \n\u001b[32m     86\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     88\u001b[39m add_request = \u001b[38;5;28mself\u001b[39m._validate_and_prepare_add_request(\n\u001b[32m     89\u001b[39m     ids=ids,\n\u001b[32m     90\u001b[39m     embeddings=embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m     uris=uris,\n\u001b[32m     95\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_add\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadatas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43muris\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muris\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bezaw\\OneDrive\\Desktop\\10Acadamy-KAIM\\RAG-Compliant-Chatbot\\rag-complaint-chatbot\\venv\\Lib\\site-packages\\chromadb\\api\\rust.py:441\u001b[39m, in \u001b[36mRustBindingsAPI._add\u001b[39m\u001b[34m(self, ids, collection_id, embeddings, metadatas, documents, uris, tenant, database)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add\u001b[39m(\n\u001b[32m    421\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    429\u001b[39m     database: \u001b[38;5;28mstr\u001b[39m = DEFAULT_DATABASE,\n\u001b[32m    430\u001b[39m ) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    431\u001b[39m     \u001b[38;5;28mself\u001b[39m.product_telemetry_client.capture(\n\u001b[32m    432\u001b[39m         CollectionAddEvent(\n\u001b[32m    433\u001b[39m             collection_uuid=\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m         )\n\u001b[32m    439\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mInternalError\u001b[39m: Error in compaction: Failed to apply logs to the metadata segment"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Load chunks if kernel restarted\n",
    "if 'chunks_with_metadata' not in globals():\n",
    "    import pickle\n",
    "    with open('data/processed/chunks_with_metadata.pkl', 'rb') as f:\n",
    "        chunks_with_metadata = pickle.load(f)\n",
    "    print(\"Loaded chunks_with_metadata from pickle file\")\n",
    "\n",
    "# Embedding function\n",
    "embedding_function = SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Persistent Chroma client\n",
    "client = chromadb.PersistentClient(path=\"vector_store/chroma_db\")\n",
    "\n",
    "# Correct method name!\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"complaints_sample\",\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "print(f\"Collection has {collection.count()} vectors before adding\")\n",
    "\n",
    "# Prepare lists\n",
    "ids = []\n",
    "documents = []\n",
    "metadatas = []\n",
    "\n",
    "for i, chunk in enumerate(chunks_with_metadata):\n",
    "    chunk_id = f\"complaint_{chunk['complaint_id']}_chunk_{chunk['chunk_index']}\"\n",
    "    ids.append(chunk_id)\n",
    "    documents.append(chunk['text'])\n",
    "    metadatas.append({\n",
    "        'complaint_id': chunk['complaint_id'],\n",
    "        'product_category': chunk['product_category'],\n",
    "        'chunk_index': chunk['chunk_index'],\n",
    "        'total_chunks': chunk['total_chunks'],\n",
    "        'source': 'sample_12k'\n",
    "    })\n",
    "\n",
    "# Add in very small batches to avoid memory spike\n",
    "batch_size = 200\n",
    "total_added = 0\n",
    "\n",
    "print(\"Starting batch insertion...\")\n",
    "for start in range(0, len(ids), batch_size):\n",
    "    end = min(start + batch_size, len(ids))\n",
    "    print(f\"Adding batch {start//batch_size + 1}: chunks {start} to {end-1}\")\n",
    "    \n",
    "    collection.add(\n",
    "        ids=ids[start:end],\n",
    "        documents=documents[start:end],\n",
    "        metadatas=metadatas[start:end]\n",
    "    )\n",
    "    \n",
    "    total_added += (end - start)\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nSuccessfully added {total_added} vectors!\")\n",
    "print(f\"Final collection count: {collection.count()}\")\n",
    "\n",
    "# Create vector_store folder if not exists\n",
    "os.makedirs(\"vector_store/chroma_db\", exist_ok=True)\n",
    "print(\"Vector store saved at: vector_store/chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48235f21",
   "metadata": {},
   "source": [
    "Quick Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa50090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Why are customers unhappy with credit card fees?\n",
      "\n",
      "Result 1 [Distance: 0.3374] - Credit Cards (Complaint 1793173)\n",
      "seems incompetent to be offering a credit card to customers....\n",
      "\n",
      "Result 2 [Distance: 0.3875] - Credit Cards (Complaint 13799302)\n",
      "i have been taken advantage by this credit card company disclosure failure on their part the credit limit was very low as { {$500.00} }. they did not disclose the annual fee of { {$120.00} } will be charged within months after the card was issued. the fee is almost of the credit limit and they will charge to added fees. the interest rate is very high at and the balance goes up to the point that it...\n",
      "\n",
      "Result 3 [Distance: 0.3879] - Credit Cards (Complaint 11348756)\n",
      "provisions enforced by the cfpb, and stated to that, according to the card act, creditors are required to provide justification of real processing costs when issuers impose late fees in excess of {$8.00}. assured me that american express encourages cardholders to \" have a voice. '' further, offered an alternative to my request to revise the {$29.00} late fee in an offer of miles added to my flyer ...\n",
      "\n",
      "Result 4 [Distance: 0.3916] - Credit Cards (Complaint 11265717)\n",
      "capital one credit card is charging me too much interest and fees for being late, even after explaining my financial problems...\n",
      "\n",
      "Result 5 [Distance: 0.3923] - Credit Cards (Complaint 13292133)\n",
      "caps on high-fee cards. if your credit card company requires you to pay fees ( such as, an annual fee or application fee ), those fees can not total more than 25 % of the initial credit limit. this is not a first year rule. it is for the account and laws regarding the issuance of credit. the charges are fees for the account and do not include penalty fees. the terminology they are trying to use el...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Why are customers unhappy with credit card fees?\"\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=5,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i in range(5):\n",
    "    doc = results['documents'][0][i]\n",
    "    meta = results['metadatas'][0][i]\n",
    "    dist = results['distances'][0][i]\n",
    "    print(f\"Result {i+1} [Distance: {dist:.4f}] - {meta['product_category']} (Complaint {meta['complaint_id']})\")\n",
    "    print(doc[:400] + \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf45bfef",
   "metadata": {},
   "source": [
    "Create Stratified Sample(10,000-15,000 complaints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613301fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target: ~12,000 complaints, proportional to product_category\n",
    "sample_size = 12000\n",
    "\n",
    "df_sample = df.groupby('product_category', group_keys=False).apply(\n",
    "    lambda x: x.sample(int(sample_size * len(x) / len(df)), replace=False)\n",
    ")\n",
    "\n",
    "# If slightly off due to rounding, adjust\n",
    "df_sample = df_sample.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "\n",
    "print(f\"Sampled {len(df_sample)} complaints\")\n",
    "print(df_sample['product_category'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
